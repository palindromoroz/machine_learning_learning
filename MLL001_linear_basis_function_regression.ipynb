{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear regression tutorial\n",
    "This notebook will introduce the basic machine learning concepts on a simple but important problem of linear basis function regression. It is inspired by the introductory chapters from the C.Bishop book \"Pattern Recognition and Machine Learning\"\n",
    "\n",
    "We will first include some basic packages and define a few useful functions that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def MSE(y,y_est):\n",
    "    \"\"\" Mean squared error function \"\"\"\n",
    "    return np.mean((y_est-y)**2)\n",
    "\n",
    "def pplot(x,y,models=''):\n",
    "    \"\"\" A handy plot function for plotting data and models \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(x,y,'o',color=(0.5,0.5,0.5))\n",
    "    legend=['data']\n",
    "    for model in models:\n",
    "        y_est=model[0](x)\n",
    "        plt.plot(x,y_est,linewidth=5.0)\n",
    "        legend.append('model %s - MSE:%0.3f' % (model[1],MSE(y,y_est)))\n",
    "    plt.legend(legend)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression analysis\n",
    "\n",
    "Regression analysis is a general problem for estimating the relationships among variables. A simple practical situation can be described in the following way. There is a set of $N$ observations $x_1,...,x_N$ of the variable $x$ and the corresponding $y_1,...,y_N$ observations of the variable $y$. The basic regression problem is then in finding the function $f(x)$ that can predict the value of the variable $y$ based on observed $x$. \n",
    "\n",
    "The function $f(x)$ is usually selected based on some optimality criterion. A common criterion we will use here is the mean squared error between the predicted and the real value of $y$:\n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_{n=1}^N (y_n-f(x_n))^2 $$\n",
    "\n",
    "As an illustration we first generate some noisy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(N,sigma,xstart=-2,xstop=2):\n",
    "    # generate some data\n",
    "    x = np.array([np.linspace(xstart, xstop, N)]).T      # x are regularly spaced \n",
    "    f_gt = lambda x: 3 + np.sin(x) + 0.5*np.sin(1.5*x-2) # some function\n",
    "    y = f_gt(x) + sigma*np.random.randn(N,1)             # add random noise\n",
    "    return x,y,f_gt\n",
    "\n",
    "x,y,f_gt = genData(N=30,sigma=0.35) # 30 points with additive noise with std of 0.35\n",
    "\n",
    "# show the data and the underlying ground truth function\n",
    "pplot(x,y,[[f_gt,'ground_truth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear basis function model\n",
    "In many practical situations the actual function $f(x)$ behind the data is unknown. Therefore the regression problem consists of two parts. First, choose a function and then as the function might have some parameters, estimate the optimal values for the parameters. Usually, for practical reasons the choice is first limited to a class of functions. A flexible and general model we consider here is the so called \"linear basis function model\".\n",
    "\n",
    "Let $\\psi_0(x),...,\\psi_{M}(x)$ be $M+1$ fixed non-linear functions of the input variable $x$. Linear combination of the non-linear functions is then:\n",
    "\n",
    "$$ f(x;{\\bf w})=\\sum_{m=0}^{M} w_m \\psi_m(x) $$\n",
    "\n",
    "where the non-linear functions $\\psi_m( x)$ are known as the \"basis functions\" and the coefficients ${\\bf w}=[w_0,...,w_M]^T$ are the model parameters. \n",
    "\n",
    "There are many possible choices for the basis functions. We will use here the polynomial basis:\n",
    "\n",
    "$$ \\psi_m( x)= x^m $$\n",
    "\n",
    "Other popular choices for basis functions are Gaussian basis, logistic functions, Fourier basis, etc.\n",
    "\n",
    "It is also common to have one of the functions equal to 1, i.e. $\\psi_0(x)=1$, such that the corresponding parameter $w_0$, also known as \"bias\", describes the global data offset.  \n",
    " \n",
    "The polynomial basis function model is defined in Python code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(x,m):\n",
    "    \"\"\" Polynomial basis function \"\"\"\n",
    "    return x**m\n",
    "\n",
    "def Psi_create_fn(x,M):\n",
    "    \"\"\" Vector of M basis functions \"\"\"\n",
    "    return lambda x: np.hstack([psi(x,m) for m in range(M+1)])\n",
    "\n",
    "def linear_model(w,Psi):\n",
    "    \"\"\" Linear basis function model \"\"\"\n",
    "    return Psi.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares model fitting\n",
    "For a chosen linear basis function model we can find the parameters $\\bf w$ that minimize the mean squared error function:\n",
    "\n",
    "$$ L({\\bf w})=\\frac{1}{N}\\sum_{n=1}^{N} (y_n-f(x_n;{\\bf w}))^2 $$\n",
    "\n",
    "This can be written also in a matrix form:\n",
    "\n",
    "$$ L({\\bf w}) = \\frac{1}{N} ({\\bf y}- \\Psi({\\bf x}){\\bf w})^T ({\\bf y}- \\Psi({\\bf x}){\\bf w})$$\n",
    "\n",
    "where ${\\bf y}=[y_1,...,y_N]^T$  and ${\\bf x}=[x_1,...,x_N]^T$ are the observed data points stacked in vectors, and  \n",
    "\n",
    "$$ \\Psi({\\bf x})=\n",
    "\\begin{bmatrix}\n",
    "    \\psi_0(x_1)  & \\psi_1(x_1) & \\dots & \\psi_M(x_1) \\\\\n",
    "    \\psi_0(x_2)  & \\psi_1(x_2) & \\dots & \\psi_M(x_2) \\\\\n",
    "    \\vdots & \\vdots &  \\ddots & \\vdots \\\\\n",
    "    \\psi_0(x_N)  & \\psi_1(x_N) & \\dots & \\psi_M(x_N) \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Setting the gradient to zero gives:\n",
    "\n",
    "$$ \\Psi({\\bf x})^T\\Psi({\\bf x}){\\bf w}=\\Psi({\\bf x})^T{\\bf y} $$\n",
    "\n",
    "This can be seen as a set of $M+1$ linear equations that can be solved to find the optimal ${\\bf w}_{est}$. One approach is the so called Moore-Penrose pseudo inverse:\n",
    "\n",
    "$$ {\\bf w}_{estimated} = (\\Psi({\\bf x})^T\\Psi({\\bf x}))^{-1} \\Psi({\\bf x})^T{\\bf y} $$\n",
    "\n",
    "While this is a typical textbook method it is also known that it is a poor method in terms of numerical stability. This will be illustrated later.\n",
    "\n",
    "The code below implements the pseudo-inverse model fitting procedure and also a better linear equations solver based on the matrix singular value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(x,y,M,method=1,printw=False):\n",
    "    \"\"\" Create an M-th order model and fit it to the data\"\"\"\n",
    "    # create matrix Psi\n",
    "    Psi_fn = Psi_create_fn(x,M)\n",
    "    Psi = Psi_fn(x)\n",
    "    \n",
    "    # two methods for parameter estimation\n",
    "    if method==1:\n",
    "        # Moore-Penrose pseudo-inverse\n",
    "        w_estimated = np.linalg.inv(Psi.T.dot(Psi)).dot(Psi.T.dot(y))\n",
    "    else :\n",
    "        # divide-and-conquer SVD (Lapack xGELSD)\n",
    "        w_estimated = np.linalg.lstsq(Psi,y,rcond=None)[0]\n",
    "    # create the estimated model function\n",
    "    model = lambda x: linear_model(w_estimated,Psi_fn(x))\n",
    "    \n",
    "    # model name\n",
    "    name = 'M=%d' % M\n",
    "    # print out the estimated coefficients\n",
    "    if printw:\n",
    "        print( name + ',w=' + ','.join('{:.2f}'.format(w) for w in np.nditer(w_estimated)))\n",
    "    return model, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Now we have all the important functions implemented and we can try them on the synthetic data and observe some important aspects of the regression analysis.\n",
    "\n",
    "### Experiment 1: Basic models\n",
    "\n",
    "Try models of order 0,1,2. This should give you the constant, linear and quadratic model. See if you get what is expected. Observe the curves and how the error is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and fit a number of models\n",
    "models=[fit_model(x,y,M,printw=True) for M in [0,1,2]]\n",
    "# show the data and the models\n",
    "pplot(x,y,models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Overfitting\n",
    "\n",
    "Add more complex models, e.g. M=20 for N=30 data points. Observe how does it look like. This is an example where model is overfitting the data. Then increase the number of data points to for example N=300. With more data te model becomes smoother and closer to the actual function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=20\n",
    "\n",
    "N=30\n",
    "x,y,f_gt = genData(N=N,sigma=0.35)\n",
    "print('With N = {} data points.'.format(N))\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True)])\n",
    "\n",
    "N=300\n",
    "x,y,f_gt = genData(N=N,sigma=0.35)\n",
    "print('With N = {} data points.'.format(N))\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Numerical problems with the pseudo-inverse solution\n",
    "\n",
    "Further increase the model complexity to for example M=30. Observe the result. Then replace the Moore-Penrose pseudo-inverse with the SVD method. The SVD method is more numerically stable and can deal even with such a complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=30\n",
    "print('Using pseudo-inverse:')\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True)])\n",
    "print('Using SVD:')\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True,method=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Further numerical problems\n",
    "\n",
    "Try even higher dimensional model, e.g. M=50. Even the SVD model fails here. This is illustrating the general problem of polynomial models with numerical stability as the model complexity increases. Complex models require huge dynamical range. Many other models that model the functions more locally are much more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=50\n",
    "print('Using SVD:')\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True,method=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "The experiments above illustrate probably the central problem in the estimation theory: how to find the model of the \"right\" complexity. The common approach for dealing with model selection is the so called \"cross validation\". One round of cross validation consist typically of dividing the data into \"training\" and \"test\" (or \"validation\") sets. The training data is used to estimate the model parameters and the test data to evaluate the generalization performance of the model. Typically multiple rounds of cross-validation are done with different data partitions. \n",
    "\n",
    "For simplicity we do here just one partition into $75\\%$ train and $25\\%$ test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(N)\n",
    "train_idx, test_idx = indices[:75*N//100], indices[75*N//100:]\n",
    "x_train, x_test = x[train_idx], x[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now try a set of models of different complexity $M$ and see how they perform on the test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train=[]\n",
    "error_test=[]\n",
    "for M in range(20):\n",
    "    model,_=fit_model(x_train,y_train,M,method=2)\n",
    "    error_train.append(MSE(y_train,model(x_train)))\n",
    "    error_test.append(MSE(y_test,model(x_test)))\n",
    "\n",
    "\n",
    "# show\n",
    "plt.figure()\n",
    "plt.plot(error_train)\n",
    "plt.plot(error_test)\n",
    "plt.xlabel('M')\n",
    "plt.legend(['error_train','error_test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that for simple models the training and testing errors are both decreasing. After certain model complexity the test error starts increasing while the error on the training data gets always lower. This behavior is known as \"over-fitting\" the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Another way of dealing with model complexity is to modify the model such that it becomes less prone to over-fitting the data. This method is known as model regularization. For our polynomial model a simple heuristic is to add additional constraints such that the weights $w_m$ are kept small and potentially set to zero if they are not needed. One way of achieving this is by adding an additional term to the optimization function:\n",
    "\n",
    "$$ L({\\bf w})=\\frac{1}{N}\\{\\sum_{n=1}^{N} (y_n-f(x_n;{\\bf w}))^2 + \\beta {\\bf w}{\\bf w}^T \\}$$\n",
    "\n",
    "Setting the derivative to zero again gives a linear set of equations:\n",
    "\n",
    "$$\\{\\beta+\\Psi({\\bf x})^T\\Psi({\\bf x})\\}{\\bf w}=\\Psi({\\bf x})^T{\\bf y} $$\n",
    "\n",
    "Let us now take a relatively complex model, e.g. M=20, and plot both the regularized and the non-regularized version and observe the difference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_regularized(x,y,M,beta,printw=False):\n",
    "    \"\"\" Create an M-th order model and fit it to the data\"\"\"\n",
    "    Psi_fn=Psi_create_fn(x,M)\n",
    "    Psi=Psi_fn(x)\n",
    "    w_estimated = np.linalg.lstsq(beta*np.eye(M+1)+Psi.T.dot(Psi),Psi.T.dot(y),rcond=None)[0]\n",
    "    # create the estimated model function\n",
    "    model = lambda x: linear_model(w_estimated,Psi_fn(x))\n",
    "    \n",
    "    # model name\n",
    "    name = 'M=%d(beta=%0.2f)' % (M,beta)\n",
    "    # print out the estimated coefficients\n",
    "    if printw:\n",
    "        print( name + ',w=' + ','.join('{:.2f}'.format(w) for w in np.nditer(w_estimated)))\n",
    "    return model, name\n",
    "\n",
    "M=20\n",
    "\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True),fit_model_regularized(x,y,M,2,printw=True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization on the new data - extrapolation\n",
    "\n",
    "Until now we fitted our models on data where the input variable $x$ was in the range $[-2,2]$. What will happen if suddenly we need to address the data in a slightly wider range, e.g. $[-2,2.1]$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new,y_new,f_gt = genData(N=300,sigma=0.35,xstart=-2,xstop=2.1)\n",
    "pplot(x_new,y_new,[[f_gt,'ground_truth'],fit_model_regularized(x,y,M=20,beta=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can collect the data in the new unseen areas we can easily reuse all the infrastructure and *retrain* the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pplot(x_new,y_new,[[f_gt,'ground_truth'],fit_model_regularized(x_new,y_new,M=20,beta=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "A generic class of linear basis function approximation models is introduced. Polynomial basis performance analyzed on a single variable toy problem.\n",
    "\n",
    "### Key take away message: \n",
    "\n",
    "***The main practical challenge in machine learning is generalizing the performance to unseen data***\n",
    "\n",
    "### Other learnings:\n",
    "\n",
    "* true model unknown\n",
    "* flexible generic classes models\n",
    "* *model selection* - selecting the right model and model complexity \n",
    "* *cross-validation* - a common way for controlling over-fitting and selecting the right model\n",
    "* *regularization* - adding additional constraints to control the model complexity \n",
    "* learning = training = parameter estimation\n",
    "* inference typically means = applying the model with *trained* parameters on the new unseen data\n",
    "* large number of data points and complex model lead to many practical numerical problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toward Neural Networks and Deep Learning\n",
    "\n",
    "Same problem and way of working but working with different models and different optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient based optimization\n",
    "\n",
    "Minimizing the loss function $L(\\bf{w})$ is done in closed form for this simple linear problem. Modern machine learning is typically dealing with much more complex models, e.g. neural networks, where there might not exist a closed form solution. Therefore generic optimization procedures are usually used. Most approaches rely on the gradient of the loss function. In the simplest form the gradient optimization procedure starts with a random choice for the parameter vector denoted as $\\bf{w}_0$. Then a number of iterations is done by updating the parameters in the opposite direction of the loss function gradient:\n",
    "\n",
    "$ \\bf{w}_{k+1}=\\bf{w}_k-\\gamma \\frac{\\delta L}{\\delta \\bf{w}} (\\bf{w}_k)$\n",
    "\n",
    "The parameter $\\gamma$ is also known as the step size. If $\\gamma$ is small enough, $L(\\bf{w}_{k+1}) \\le  L(\\bf{w}_{k})$. Modern machine learning frameworks include many variations of the gradient based optimization and also perform automatic calculations of the gradients. Typically and you need to define a model and loss function and many optimization procedures can then used. To illustrate this we will use TensorFlow below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # to run this cell you will need to install TensorFlow\n",
    "\n",
    "def fit_model_regularized_tf(x,y,M,beta=0,printw=False,nIterations=1000,gamma=0.01):\n",
    "    \"\"\" Create an M-th order model and fit it to the data\"\"\"\n",
    "    \n",
    "    # model\n",
    "    tf.reset_default_graph()\n",
    "    Psi_fn = Psi_create_fn(x,M)\n",
    "    Psi = tf.constant(Psi_fn(x),dtype=tf.float32)\n",
    "    y_t = tf.constant(y)\n",
    "    w   = tf.get_variable('w',[M+1,1])\n",
    "    y_p = tf.matmul(Psi,w)\n",
    "    \n",
    "    # define loss function\n",
    "    loss = tf.losses.mean_squared_error(labels=y_t, predictions=y_p) + beta* tf.matmul(tf.transpose(w),w)\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(gamma)\n",
    "    train     = optimizer.minimize(loss)\n",
    "    \n",
    "    # run the optimizer\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(nIterations):\n",
    "        _, loss_value,w_estimated = sess.run((train, loss,w))\n",
    "        # print(loss_value)\n",
    "    \n",
    "    # create the estimated model function\n",
    "    model = lambda x: linear_model(w_estimated,Psi_fn(x))\n",
    "    \n",
    "    # model name\n",
    "    name = 'M=%d(beta=%0.2f)(tf)' % (M,beta)\n",
    "    # print out the estimated coefficients\n",
    "    if printw:\n",
    "        print( name + ',w=' + ','.join('{:.2f}'.format(w) for w in np.nditer(w_estimated)))\n",
    "    return model, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Compare to the closed form optimization\n",
    "\n",
    "Use small model, e.g. M=3, and change the \"hyper-parameters\" $\\gamma$ and number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=3\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True),fit_model_regularized_tf(x,y,M,gamma=0.01,printw=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change to a more complex model, e.g. M=5, and observe how change the *hyper-parameters*  $\\gamma$  and number of iterations will change the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=5\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True),fit_model_regularized_tf(x,y,M,gamma=0.001,nIterations=1000,printw=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take away messages:\n",
    "\n",
    "* gradient descent is generic method that can be applied to any differentiable function of arbitrary complexity\n",
    "* the performance of the gradient descent can be poor and convergence needs to be controlled via a set of *hyper-parameters*\n",
    "* TensorFlow and other modern machine learning tools work with compute graphs. Typical flow is:\n",
    "    * define compute graph\n",
    "    * define operation on the graph\n",
    "    * map the graph on the compute HW\n",
    "    * run the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model\n",
    "\n",
    "Perceptron function:\n",
    "\n",
    "$$ f_a(\\sum_{m=0}^{M} w_m x_m) $$\n",
    "\n",
    "where $f_a$ is known as the \"activation function\" and it is usually a non-linear function, e.g. hyperbolic tangent $\\tanh$.\n",
    "\n",
    "![alt_text](images/Sinh_cosh_tanh.svg)\n",
    "\n",
    "A neural network for simple single variable regression function could be:\n",
    "\n",
    "$$ f(x;{\\bf w})=\\sum_{m=0}^{M} w_m \\tanh(a_m x+ b_m) $$\n",
    "\n",
    "where the coefficients ${\\bf w}=[w_0,a_0,b_0,...,w_M,a_M,b_M]^T$ are now the model parameters. This can be seen as a 2 layer network.\n",
    "\n",
    "Linear basis function model as a neural network:\n",
    "\n",
    "![alt text](images/linear_as_NN.svg)\n",
    "\n",
    "Simple neural network for single variable regression:\n",
    "\n",
    "![alt text](images/NN_for_regression.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the model using high level TensorFlow API called Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def fit_model_nn(x,y,M,printw=False,nIterations=1000,gamma=0.1):\n",
    "    \"\"\" Create an M-th order model and fit it to the data\"\"\"\n",
    "    \n",
    "    # model\n",
    "    input  = keras.Input(shape=(1,),name='x')\n",
    "    layer  = keras.layers.Dense(M,activation='tanh',name='Layer1')(input)\n",
    "    layer  = keras.layers.Dense(1,activation='linear',use_bias=False,name='Layer2')(layer)\n",
    "    model  = keras.Model(inputs=input, outputs=layer)\n",
    "    \n",
    "    # optimizer\n",
    "    model.compile(optimizer=keras.optimizers.SGD(lr=gamma),loss='mse')\n",
    "    \n",
    "    # run\n",
    "    model.fit(x,y,epochs=nIterations,verbose=0)\n",
    "    \n",
    "    # create the estimated model function\n",
    "    model_ = lambda x: model.predict(x)\n",
    "    \n",
    "    # model name\n",
    "    name = 'M=%d(nn)' % (M)  \n",
    "    # print out the estimated coefficients\n",
    "    if printw:\n",
    "        print('NN model:')\n",
    "        print(model.summary())\n",
    "        for layer in model.layers:\n",
    "            for w_estimated in layer.get_weights():\n",
    "                print( name +layer.name+',w=' + ','.join('{:.2f}'.format(w) for w in np.nditer(w_estimated)))\n",
    "\n",
    "    return model_, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model on the data and comparing to the polynomial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=10\n",
    "pplot(x,y,[fit_model(x,y,M,printw=True),fit_model_nn(x,y,M,printw=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Although it has 3x more parameters the neural network model seems more stable than the polynomial. \n",
    "* The gradient procedure works much better on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization on the new data - extrapolation\n",
    "\n",
    "Lets try a slightly wider range for the input variable $x$ as before from $[-2,2]$ to $[-2,2.1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new,y_new,f_gt = genData(N=300,sigma=0.35,xstart=-2,xstop=2.1)\n",
    "pplot(x_new,y_new,[[f_gt,'ground_truth'],fit_model_nn(x,y,M)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now try a significantly wider range for the input variable $x$ as before from $[-2,2]$ to $[-2,4]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new,y_new,f_gt = genData(N=300,sigma=0.35,xstart=-2,xstop=4)\n",
    "pplot(x_new,y_new,[[f_gt,'ground_truth'],fit_model_nn(x,y,M)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can collect the data in the new unseen areas we can easily reuse all the infrastructure and *retrain* the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pplot(x_new,y_new,[[f_gt,'ground_truth'],fit_model_nn(x_new,y_new,M)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "Perceptron - single layer:\n",
    "- Warren McCulloch and Walter Pitts  (1943), \"A Logical Calculus of Ideas Immanent in Nervous Activity\"\n",
    "- Frank Rosenblatt (1957), \"The Perceptron--a perceiving and recognizing automaton\"\n",
    "[perceptron](images/Mark_I_perceptron.jpeg) Source(Wikipedia)\n",
    "\n",
    "Neural networks - multiple layers:\n",
    "- Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). \"Learning representations by back-propagating errors\"\n",
    "- George Cybenko (1989), \"Approximation by superpositions of a sigmoidal function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key take away message: \n",
    "\n",
    "***The main practical challenge in machine learning is generalizing the performance to unseen data***\n",
    "\n",
    "### Other learnings:\n",
    "\n",
    "* Deep learning - neural networks: more generic and more complex\n",
    "* *defining model* = *defining network topology/architecture*\n",
    "* Generic optimization tools - work well *usually*\n",
    "* Compared to other models neural:\n",
    "    * Pro: flexible and easy to scale\n",
    "    * Con: usually not optimal in terms of computation and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
