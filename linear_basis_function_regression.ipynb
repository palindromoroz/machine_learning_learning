{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear regression tutorial\n",
    "This notebook will introduce some basic machine learning concepts on the simple but important problem of linear basis function regression. It is inspired by the C.Bishop book \"Pattern Recognition and Machine Learning\"\n",
    "\n",
    "We will first include some basic packages and define a few usefull functions that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def MSE(y,y_est):\n",
    "    \"\"\" Mean squared error function \"\"\"\n",
    "    return np.mean((y_est-y)**2)\n",
    "\n",
    "def pplot(x,y,models=''):\n",
    "    \"\"\" A handy plot function for plotting data and models \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(x,y,'o',color=(0.5,0.5,0.5))\n",
    "    legend=['data']\n",
    "    for model in models:\n",
    "        y_est=model[0](x)\n",
    "        plt.plot(x,y_est,linewidth=5.0)\n",
    "        legend.append('model %s - MSE:%0.3f' % (model[1],MSE(y,y_est)))\n",
    "    plt.legend(legend)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression analysis\n",
    "Regression analysis is commonly addressed general problem for estimating the relationships among variables. Simple practical situation is when there is a set of $N$ observations $x_1,...,x_N$ of the variable $x$ and the corresponding $y_1,...,y_N$ observations of the variable $y$. The regression problem is then in finding the function $f(x)$ that can predict the value of the variable $y$. The selection of the function $f(x)$ is done based on some optimality criterion. A common criterion we will use here is the mean squared error:\n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_{n=1}^N (y_n-f(x_n))^2 $$\n",
    "\n",
    "As an illustration we first generate some noisy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data\n",
    "N = 50 # number of data points\n",
    "sigma = 0.35 # standard deviation of the additive Gaussain noise\n",
    "x = np.array([np.linspace(-2, 2, N)]).T # x are on a regularly spaced grid \n",
    "f_gt = lambda x: 3 + np.sin(x) + 0.5*np.sin(1.5*x-2) # some function\n",
    "y = f_gt(x) + sigma*np.random.randn(N,1)\n",
    "\n",
    "# show the data and the underlying ground truth function\n",
    "pplot(x,y,[[f_gt,'ground_truth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear basis function model\n",
    "We do not know the real function behind the data in most practical situations. Therefore we need to chose a function. Usually, for practical reasons the choice is first limited to a class of functions. A flexible and general model we consider here is the so called \"linear basis function model\".\n",
    "\n",
    "Let $\\psi_0(x),...,\\psi_{M}(x)$ be $M+1$ fixed non-linear functions of the input variable $x$. Linear combination of the non-linear functions is then:\n",
    "\n",
    "$$ f(x;{\\bf w})=\\sum_{m=0}^{M} w_m \\psi_m(x) $$\n",
    "\n",
    "where the non-linear functions $\\psi_m( x)$ are known as the \"basis functions\" and the coefficients ${\\bf w}=[w_0,...,w_M]^T$ are the model parameters. \n",
    "\n",
    "There are many possible choices for the basis functions. We will use here the polynomial basis:\n",
    "\n",
    "$$ \\psi_m( x)= x^m $$\n",
    "\n",
    "Other popular choices for basis functions are Gaussian basis, logistic functions, Fourier basis, etc.\n",
    "\n",
    "It is also common to have one of the functions equal to 1, i.e. $\\psi_0(x)=1$, such that the corresponding parameter $w_0$, also known as \"bias\", describes the global data offset.  \n",
    " \n",
    "The polynomial basis function model is defined in Python code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(x,m):\n",
    "    \"\"\" Polynomial basis function \"\"\"\n",
    "    return x**m\n",
    "\n",
    "\n",
    "def Psi_create_fn(x,M):\n",
    "    \"\"\" Vector of M basis functions \"\"\"\n",
    "    return lambda x: np.hstack([psi(x,m) for m in range(M+1)])\n",
    "\n",
    "\n",
    "def linear_model(w,Psi):\n",
    "    \"\"\" Linear basis function model \"\"\"\n",
    "    return Psi.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares model fitting\n",
    "For a chosen linear basis function model we can find the parameters $\\bf w$ that minimize the mean squared error function:\n",
    "\n",
    "$$ L({\\bf w})=\\frac{1}{N}\\sum_{n=1}^{N} (y_n-f(x_n;{\\bf w}))^2 $$\n",
    "\n",
    "This can be written also in a matrix form:\n",
    "\n",
    "$$ L({\\bf w}) = \\frac{1}{N} ({\\bf y}- \\Psi({\\bf x}){\\bf w})^T ({\\bf y}- \\Psi({\\bf x}){\\bf w})$$\n",
    "\n",
    "where ${\\bf y}=[y_1,...,y_N]^T$  and ${\\bf x}=[x_1,...,x_N]^T$ are the observed data points stacked in vectors, and  \n",
    "\n",
    "$$ \\Psi({\\bf x})=\n",
    "\\begin{bmatrix}\n",
    "    \\psi_0(x_1)  & \\psi_1(x_1) & \\dots & \\psi_M(x_1) \\\\\n",
    "    \\psi_0(x_2)  & \\psi_1(x_2) & \\dots & \\psi_M(x_2) \\\\\n",
    "    \\vdots & \\vdots &  \\ddots & \\vdots \\\\\n",
    "    \\psi_0(x_N)  & \\psi_1(x_N) & \\dots & \\psi_M(x_N) \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Setting the gradient to zero gives:\n",
    "\n",
    "$$ \\Psi({\\bf x})^T\\Psi({\\bf x}){\\bf w}=\\Psi({\\bf x})^T{\\bf y} $$\n",
    "\n",
    "This can be seen as a set of $M+1$ linear equations that can be solved to find the optimal ${\\bf w}_{est}$. One approach is the so called Moore-Penrose pseudo inverse:\n",
    "\n",
    "$$ {\\bf w}_{estimated} = (\\Psi({\\bf x})^T\\Psi({\\bf x}))^{-1} \\Psi({\\bf x})^T{\\bf y} $$\n",
    "\n",
    "While this is a typical textbook method it is also known that it is a particulary poor method in terms of numerical stability. This will be illustrated later.\n",
    "\n",
    "The code below implements the model fitting procedure and includes also a better linear equations solver based on matrix singular value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(x,y,M,method=1,printw=False):\n",
    "    \"\"\" Create an M-th order model and fit it to the data\"\"\"\n",
    "    # create matrix Psi\n",
    "    Psi_fn = Psi_create_fn(x,M)\n",
    "    Psi = Psi_fn(x)\n",
    "    if method==1:\n",
    "        # Moore-Penrose pseudo-inverse\n",
    "        w_estimated = np.linalg.inv(Psi.T.dot(Psi)).dot(Psi.T.dot(y))\n",
    "    else :\n",
    "        # divide-and-conquer SVD (Lapack xGELSD)\n",
    "        w_estimated = np.linalg.lstsq(Psi,y,rcond=None)[0]\n",
    "    # model name\n",
    "    name = 'M=%d' % M\n",
    "    # print out the estimated coefficients\n",
    "    if printw:\n",
    "        print( name + ',w=' + ','.join('{:.2f}'.format(w) for w in np.nditer(w_estimated)))\n",
    "    # create the estimated model function\n",
    "    model = lambda x: linear_model(w_estimated,Psi_fn(x))\n",
    "    return model, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Now we have all the important functions implemented and we can try them on the synthetic data and observe some important aspects the regression analysis.\n",
    "\n",
    "Some suggested experiments to do here are:\n",
    "\n",
    "1. Try models of order 0,1,2. This should give you the constant, linear and quadratic model. See if you get what is expected.\n",
    "\n",
    "2. Add more complex models, e.g. M=15. Observe how does it look like. Go above to the data generation step and increase the number of data points to for example N=150. Fit again the M=15 model and observe the results.\n",
    "\n",
    "3. Further increase the model complexity to for example M=30. Observe the result. Then replace the Moore-Penrose pseudo-inverse with the SVD method.\n",
    "\n",
    "4. Try even higher dimensional model, e.g. M=50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and fit a number of models\n",
    "models=[fit_model(x,y,M,printw=True) for M in [0,1,2]]\n",
    "\n",
    "# show the data and the models\n",
    "pplot(x,y,models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "The experiments above illustrate probably the central problem in estimation therory, the problem of the model of the \"right\" complexity. The most common approach for dealing with model selection is the so called \"cross validation\". One round of cross validation consist typically of dividing the data into \"training\" and \"test\" (or \"validation\") sets. The training data is used to estimate the model paramaters and the test data to evaluate the generalization perfromance of the model. Typically multiple rounds of cross-validation are done with different data partitions. For simplicity we do here just one partition into $75\\%$ train and $25\\%$ test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(N)\n",
    "train_idx, test_idx = indices[:75*N//100], indices[75*N//100:]\n",
    "x_train, x_test = x[train_idx], x[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now try a set of models of different complexity $M$ and see how they perfrom on the test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train=[]\n",
    "error_test=[]\n",
    "for M in range(15):\n",
    "    model,_=fit_model(x_train,y_train,M)\n",
    "    error_train.append(MSE(y_train,model(x_train)))\n",
    "    error_test.append(MSE(y_test,model(x_test)))\n",
    "\n",
    "\n",
    "# show\n",
    "plt.figure()\n",
    "plt.plot(error_train)\n",
    "plt.plot(error_test)\n",
    "plt.xlabel('M')\n",
    "plt.legend(['error_train','error_test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that for simple models the training and testing errors are both decresing. After certain model complexity the test error starts increasing while the error on the training data gets always lower. This behaviour is known as \"overfitting\" the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Another way of dealing with model complexity is to modify the model such that it becomes less prone to overfitting the data. This method is known as model regularization. For our polynomial model a simple heuristic is to add additional constraints such that the weights $w_m$ are kept small and potentially set to zero if they are not needed. One way of achieving this is by adding an additional term to the optimization function:\n",
    "\n",
    "$$ L({\\bf w})=\\frac{1}{N}\\{\\sum_{n=1}^{N} (y_n-f(x_n;{\\bf w}))^2 + \\beta {\\bf w}{\\bf w}^T \\}$$\n",
    "\n",
    "Setting the derivative to zero again gives a linear set of equations:\n",
    "\n",
    "$$\\{\\beta+\\Psi({\\bf x})^T\\Psi({\\bf x})\\}{\\bf w}=\\Psi({\\bf x})^T{\\bf y} $$\n",
    "\n",
    "Let us now take a relatively complex model, e.g. M=20, and plot both the regularized and the non-regularized version and observe the difference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_regularized(x,y,M,beta,printw=False):\n",
    "    \"\"\" Create an M-th order model and fit it to the data\"\"\"\n",
    "    Psi_fn=Psi_create_fn(x,M)\n",
    "    Psi=Psi_fn(x)\n",
    "    w_estimated = np.linalg.lstsq(beta*np.eye(M+1)+Psi.T.dot(Psi),Psi.T.dot(y),rcond=None)[0]\n",
    "    # model name\n",
    "    name = 'M=%d(beta=%0.2f)' % (M,beta)\n",
    "    # print out the estimated coefficients\n",
    "    if printw:\n",
    "        print( name + ',w=' + ','.join('{:.2f}'.format(w) for w in np.nditer(w_estimated)))\n",
    "    # create the estimated model function\n",
    "    model = lambda x: linear_model(w_estimated,Psi_fn(x))\n",
    "    return model, name\n",
    "    \n",
    "\n",
    "pplot(x,y,[fit_model(x,y,20,printw=True),fit_model_regularized(x,y,20,0.9,printw=True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The most common task in machine learning is fitting a model to some data. Besides the optimization procedure the central problem in machine learning is selecting the right model. This tutorial gives some hand-on experience on a simple linear basis function model. Main concepts and approaches are illistrated. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
